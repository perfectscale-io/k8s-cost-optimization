{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes Cost Optimization Welcome to the Kubernetes Cost Optimization Guide This website provides the learning materials for engineers wishing to optimize their K8s cluster costs without compromizing performance and reliability. If you find issues with any of the website materials - please send us a note. Browse our guides: The Golden Signals of Kubernetes Cost Optimization The Economics of Kubernetes Resource Allocation The 4 Areas of Kubernetes Cost Optimization Balancing Cost with Performance and Reliability Kubernetes Workload Rightsizing Pod Autoscaling Cluster Autoscaling Leveraging Cloud Discounts","title":"Home"},{"location":"#kubernetes-cost-optimization","text":"","title":"Kubernetes Cost Optimization"},{"location":"#welcome-to-the-kubernetes-cost-optimization-guide","text":"This website provides the learning materials for engineers wishing to optimize their K8s cluster costs without compromizing performance and reliability. If you find issues with any of the website materials - please send us a note. Browse our guides: The Golden Signals of Kubernetes Cost Optimization The Economics of Kubernetes Resource Allocation The 4 Areas of Kubernetes Cost Optimization Balancing Cost with Performance and Reliability Kubernetes Workload Rightsizing Pod Autoscaling Cluster Autoscaling Leveraging Cloud Discounts","title":"Welcome to the Kubernetes Cost Optimization Guide"},{"location":"cloud-discounts/","text":"Leveraging Cloud Discounts Spot VMs and Best Effort Pods Combining Spot and On-Demand Fleets Applying Saving Plans to your Kubernetes clusters","title":"Leveraging Cloud Discounts"},{"location":"cloud-discounts/#leveraging-cloud-discounts","text":"Spot VMs and Best Effort Pods Combining Spot and On-Demand Fleets Applying Saving Plans to your Kubernetes clusters","title":"Leveraging Cloud Discounts"},{"location":"cluster-autoscaling/","text":"Cluster Autoscaling Cluster-autoscaler Karpenter Improving Kubernetes Bin Packing","title":"Cluster Autoscaling"},{"location":"cluster-autoscaling/#cluster-autoscaling","text":"Cluster-autoscaler Karpenter Improving Kubernetes Bin Packing","title":"Cluster Autoscaling"},{"location":"cost-perf-r9y/","text":"Balancing Cost with Performance and Reliability Kubernetes cost optimization comes down to pinpointing the correct resource allocations and auto-scaling factors for our workloads. But \"correct\" in this context doesn't mean \"the least possible amount of resources\". It's a delicate interplay of cost vs. performance vs.reliability. In order to run our clusters in the most cost-effective way without compromising either performance or reliability it's vitally inportant to understand the Pod QoS model and the implications of PodDisruptionBudget. Understanding the Pod QoS Model Kubernetes cost optimization starts with correct resource allocation for application containers that Kubernetes orchestrates. Each container can have requests and limits defined for either memory or cpu or both. PodDisruptionBudget and application disruption","title":"Balancing Cost with Performance and Reliability"},{"location":"cost-perf-r9y/#balancing-cost-with-performance-and-reliability","text":"Kubernetes cost optimization comes down to pinpointing the correct resource allocations and auto-scaling factors for our workloads. But \"correct\" in this context doesn't mean \"the least possible amount of resources\". It's a delicate interplay of cost vs. performance vs.reliability. In order to run our clusters in the most cost-effective way without compromising either performance or reliability it's vitally inportant to understand the Pod QoS model and the implications of PodDisruptionBudget.","title":"Balancing Cost with Performance and Reliability"},{"location":"cost-perf-r9y/#understanding-the-pod-qos-model","text":"Kubernetes cost optimization starts with correct resource allocation for application containers that Kubernetes orchestrates. Each container can have requests and limits defined for either memory or cpu or both. PodDisruptionBudget and application disruption","title":"Understanding the Pod QoS Model"},{"location":"golden-signals/","text":"The Golden Signals The 4 \"golden signals\" of Kubernetes Cost Optimization as defined in a whitepaper released by Google Cloud in June 2023. Signal Group 1.Workload Rightsizing Resources 2.Demand-based Downscaling 3.Cluster Bin Packing 4.Cloud Provider Discount Coverage Cloud Discounts These signals help us apply and measure cost optimization for Kubernets clusters. The 3 signals in the resources group apply to all clusters - be it on-prem or on-cloud. The cloud discounts naturally only apply to cloud-based managed clusters, where it is very important to pinpoint the instance types and reservation level of our cluster nodes. Let's explain each signal in a bit more detail. The Resources Group Signal Explanation Workload Rightsizing Refers to our ability to allocate the amount of resources that the workloads actually need and adapt resource requests and limits as application requirements change. Demand based autoscaling Measures the capacity of developers and platform admins to make clusters scale down during off-peak hours. Cluster bin packing Refers to our ability to measure and utilize the CPU and memory of each node in the most effective and reliable way through correct Pod placement. The Cloud Discounts group Signal Explanation Cloud Discount Coverage Refers to leveraging cloud VM instances that offer discounts, such as Spot VMs, as well as the ability of budget owners and FinOps professionals to take advantage of long-term continuous use discounts offered by cloud providers.","title":"The Golden Signals of Kubernetes Cost Optimization"},{"location":"golden-signals/#the-golden-signals","text":"The 4 \"golden signals\" of Kubernetes Cost Optimization as defined in a whitepaper released by Google Cloud in June 2023. Signal Group 1.Workload Rightsizing Resources 2.Demand-based Downscaling 3.Cluster Bin Packing 4.Cloud Provider Discount Coverage Cloud Discounts These signals help us apply and measure cost optimization for Kubernets clusters. The 3 signals in the resources group apply to all clusters - be it on-prem or on-cloud. The cloud discounts naturally only apply to cloud-based managed clusters, where it is very important to pinpoint the instance types and reservation level of our cluster nodes. Let's explain each signal in a bit more detail.","title":"The Golden Signals"},{"location":"golden-signals/#the-resources-group","text":"Signal Explanation Workload Rightsizing Refers to our ability to allocate the amount of resources that the workloads actually need and adapt resource requests and limits as application requirements change. Demand based autoscaling Measures the capacity of developers and platform admins to make clusters scale down during off-peak hours. Cluster bin packing Refers to our ability to measure and utilize the CPU and memory of each node in the most effective and reliable way through correct Pod placement.","title":"The Resources Group"},{"location":"golden-signals/#the-cloud-discounts-group","text":"Signal Explanation Cloud Discount Coverage Refers to leveraging cloud VM instances that offer discounts, such as Spot VMs, as well as the ability of budget owners and FinOps professionals to take advantage of long-term continuous use discounts offered by cloud providers.","title":"The Cloud Discounts group"},{"location":"over-under-idle-waste/","text":"The 4 Focus Areas When starting out with Kubernetes Cost Optimization it's important to understand what to focus on. Redundant costs come from 2 main sources: wasted resources and idle resources . Both of these are usually caused by over-provisioning , intentional or unintentional. On the other hand - thoughtless cost reduction activity can lead to under-provisioning , which causes performance and reliability issues. When optimizing our cluster costs we want to focus on all of these areas iteratively - in order to keep our clusters as cost-effective and performant as needed. Now let's explain each of these focus areas in more detail. Wasted Resources Wasted resources are the resources that have been allocated but not utilized. In most unoptimizaed clusters we're observing up to 50% of waste, which translates to thousands of dollars or euros monthly. This waste comes from over-provisioning the containers in our pods. Read on to understand the reasons and perils of over-provsioning. Idle Resources Kubernetes comes with a promise of automatic bin packing. I.e - it is supposed to fit the largest possible amount of pods on every node in the cluster. But this is again dependent on engineers correctly defining 1) resource requests and 2) node sizes. Even with smart and well-tuned autoscaling tools like Karpenter this doesn't always work and we find ourselves with nodes that are more than half empty - with resources that were neither requested nor utilized. All these are idle resources and taking care of reducing them is an important focus area of Kubernetes Cost Optimization. Overprovisioning Pinpointing the exact memory and CPU requests for our pods is hard - it requires observing the application behaviour under production load over a significant time period. Therefore most engineers prefer to err towards overprovisioning - i.e setting requests much higher than the application will ever use. This leads to a large amount of allocated but unutilized resources all across the cluster. Just imagine your cluster runs 200 pods and each of them requests 100Mb more memory than it actually uses. Altogether you'll have 20Gb of wasted RAM across the cluster. These resources will be provisioned, paid for, but never actually used. Underprovisioning Container underprovisioning in Kubernetes occurs when the resources allocated to containers are insufficient to meet the demands of the applications they run. This scenario can arise from: underestimating the resource needs of an application excessively aggressive optimization based on incomplete data not setting resource requests altogether - which gives us BestEffort QoS pods Underprovisioning can lead to several issues, including poor application performance, increased latency, and even outages as containers are killed on OOM or evicted because of resource exhaustion. This can be especially problematic in production environments where reliability and responsiveness are critical. Browse the rest of our guides to learm how to address each one of these focus areas in your Kubernetes cost optimization effort.","title":"The 4 Focus Areas of Kubernetes Cost Optimization"},{"location":"over-under-idle-waste/#the-4-focus-areas","text":"When starting out with Kubernetes Cost Optimization it's important to understand what to focus on. Redundant costs come from 2 main sources: wasted resources and idle resources . Both of these are usually caused by over-provisioning , intentional or unintentional. On the other hand - thoughtless cost reduction activity can lead to under-provisioning , which causes performance and reliability issues. When optimizing our cluster costs we want to focus on all of these areas iteratively - in order to keep our clusters as cost-effective and performant as needed. Now let's explain each of these focus areas in more detail.","title":"The 4 Focus Areas"},{"location":"over-under-idle-waste/#wasted-resources","text":"Wasted resources are the resources that have been allocated but not utilized. In most unoptimizaed clusters we're observing up to 50% of waste, which translates to thousands of dollars or euros monthly. This waste comes from over-provisioning the containers in our pods. Read on to understand the reasons and perils of over-provsioning.","title":"Wasted Resources"},{"location":"over-under-idle-waste/#idle-resources","text":"Kubernetes comes with a promise of automatic bin packing. I.e - it is supposed to fit the largest possible amount of pods on every node in the cluster. But this is again dependent on engineers correctly defining 1) resource requests and 2) node sizes. Even with smart and well-tuned autoscaling tools like Karpenter this doesn't always work and we find ourselves with nodes that are more than half empty - with resources that were neither requested nor utilized. All these are idle resources and taking care of reducing them is an important focus area of Kubernetes Cost Optimization.","title":"Idle Resources"},{"location":"over-under-idle-waste/#overprovisioning","text":"Pinpointing the exact memory and CPU requests for our pods is hard - it requires observing the application behaviour under production load over a significant time period. Therefore most engineers prefer to err towards overprovisioning - i.e setting requests much higher than the application will ever use. This leads to a large amount of allocated but unutilized resources all across the cluster. Just imagine your cluster runs 200 pods and each of them requests 100Mb more memory than it actually uses. Altogether you'll have 20Gb of wasted RAM across the cluster. These resources will be provisioned, paid for, but never actually used.","title":"Overprovisioning"},{"location":"over-under-idle-waste/#underprovisioning","text":"Container underprovisioning in Kubernetes occurs when the resources allocated to containers are insufficient to meet the demands of the applications they run. This scenario can arise from: underestimating the resource needs of an application excessively aggressive optimization based on incomplete data not setting resource requests altogether - which gives us BestEffort QoS pods Underprovisioning can lead to several issues, including poor application performance, increased latency, and even outages as containers are killed on OOM or evicted because of resource exhaustion. This can be especially problematic in production environments where reliability and responsiveness are critical. Browse the rest of our guides to learm how to address each one of these focus areas in your Kubernetes cost optimization effort.","title":"Underprovisioning"},{"location":"pod-autoscaling/","text":"Pod Autoscaling Horizontal HPA CPU Memory Custom metrics KEDA Vertical VPA Goldilocks","title":"Pod Autoscaling"},{"location":"pod-autoscaling/#pod-autoscaling","text":"Horizontal HPA CPU Memory Custom metrics KEDA Vertical VPA Goldilocks","title":"Pod Autoscaling"},{"location":"resources/","text":"The Economics of Kubernetes Resource Allocation This whole guide talks a lot about resources which is a highly overloaded word in Kubernetes world. All the objects defined in Kubernetes API (such as Pod, Service, ConfigMap, etc) are also called resources . But we're not referring to them here. So in order to make things clearer let's define resources for the purpose of this guide. When we say resources - we actually mean CPU, GPU, memory, network and storage. In the pre-cloud world all these needed to be defined in advance. Ordering and provisioning these resources took weeks or even months. In cloud native environments (i.e Kubernetes) these resources are highly dynamic in nature and can be allocated and released on demand - just by issuing an API call. The process of allocating additional resources in such an automated manner is called autoscaling. This automation gives us a lot of power by prividing access to addtional resources when needed (aka just-in-time provisioning). And it also creates undesirable artifacts if not configured correctly such as: Wasted resources (when we allocate more than we actually need) Reliability issues (when provisioning doesn't work as expected) Unexpected costs (when we don't have good control over what and when gets provisioned) Resource Allocation in Kubernetes Kubernetes has different ways of allocating, limiting and provisioning resources for application containers. CPU and memory allocation On the very basic level - engineers can request CPU and memory for a container by defining its resource requests and limits in the Pod resource spec: apiVersion: v1 kind: Pod metadata: name: example spec: containers: - image: perfectscale.io/example name: example resources: requests: cpu: 1 memory: 500Mi limits: cpu: 1 memory: 500Mi Storage Resource Allocation In order to allocate storage Kubernetes allows us to use its PersistentVolume allocation mechanisms in conjunction with one of the multiple supported storage providers (e.g OpenEBS, Portworx, etc.) Network Resource Allocation Network resources aren't managed by Kubernetes itself but instead are delegated to one of the multiple CNI networking providers, which are reponsible for provisioning, allocating and retiring network interfaces and addresses. Continue here to get a better understanding of the 4 main focus areas of Kubernetes Cost Optimization .","title":"The Economics of Kubernetes Resource Allocation"},{"location":"resources/#the-economics-of-kubernetes-resource-allocation","text":"This whole guide talks a lot about resources which is a highly overloaded word in Kubernetes world. All the objects defined in Kubernetes API (such as Pod, Service, ConfigMap, etc) are also called resources . But we're not referring to them here. So in order to make things clearer let's define resources for the purpose of this guide. When we say resources - we actually mean CPU, GPU, memory, network and storage. In the pre-cloud world all these needed to be defined in advance. Ordering and provisioning these resources took weeks or even months. In cloud native environments (i.e Kubernetes) these resources are highly dynamic in nature and can be allocated and released on demand - just by issuing an API call. The process of allocating additional resources in such an automated manner is called autoscaling. This automation gives us a lot of power by prividing access to addtional resources when needed (aka just-in-time provisioning). And it also creates undesirable artifacts if not configured correctly such as: Wasted resources (when we allocate more than we actually need) Reliability issues (when provisioning doesn't work as expected) Unexpected costs (when we don't have good control over what and when gets provisioned)","title":"The Economics of Kubernetes Resource Allocation"},{"location":"resources/#resource-allocation-in-kubernetes","text":"Kubernetes has different ways of allocating, limiting and provisioning resources for application containers.","title":"Resource Allocation in Kubernetes"},{"location":"resources/#cpu-and-memory-allocation","text":"On the very basic level - engineers can request CPU and memory for a container by defining its resource requests and limits in the Pod resource spec: apiVersion: v1 kind: Pod metadata: name: example spec: containers: - image: perfectscale.io/example name: example resources: requests: cpu: 1 memory: 500Mi limits: cpu: 1 memory: 500Mi","title":"CPU and memory allocation"},{"location":"resources/#storage-resource-allocation","text":"In order to allocate storage Kubernetes allows us to use its PersistentVolume allocation mechanisms in conjunction with one of the multiple supported storage providers (e.g OpenEBS, Portworx, etc.)","title":"Storage Resource Allocation"},{"location":"resources/#network-resource-allocation","text":"Network resources aren't managed by Kubernetes itself but instead are delegated to one of the multiple CNI networking providers, which are reponsible for provisioning, allocating and retiring network interfaces and addresses. Continue here to get a better understanding of the 4 main focus areas of Kubernetes Cost Optimization .","title":"Network Resource Allocation"},{"location":"rightsizing/","text":"Kubernetes Workload Rightsizing Requests Memory CPU Limits Memory CPU Understanding CPU throttling Defining resource guardrails LimitRange NamespaceQuota","title":"Kubernetes Workload Rightsizing"},{"location":"rightsizing/#kubernetes-workload-rightsizing","text":"Requests Memory CPU Limits Memory CPU Understanding CPU throttling Defining resource guardrails LimitRange NamespaceQuota","title":"Kubernetes Workload Rightsizing"}]}