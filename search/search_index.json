{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Kubernetes Cost Optimization Welcome to the Kubernetes Cost Optimization Guide This website provides the learning materials for engineers wishing to optimize their K8s cluster costs without compromizing performance and reliability. If you find issues with any of the website materials - please send us a note. Browse our guides: The Golden Signals of Kubernetes Cost Optimization The Economics of Kubernetes Resource Allocation The 4 Areas of Kubernetes Cost Optimization Balancing Cost with Performance and Reliability Kubernetes Workload Rightsizing Pod Autoscaling Cluster Autoscaling Leveraging Cloud Discounts","title":"Home"},{"location":"#kubernetes-cost-optimization","text":"","title":"Kubernetes Cost Optimization"},{"location":"#welcome-to-the-kubernetes-cost-optimization-guide","text":"This website provides the learning materials for engineers wishing to optimize their K8s cluster costs without compromizing performance and reliability. If you find issues with any of the website materials - please send us a note. Browse our guides: The Golden Signals of Kubernetes Cost Optimization The Economics of Kubernetes Resource Allocation The 4 Areas of Kubernetes Cost Optimization Balancing Cost with Performance and Reliability Kubernetes Workload Rightsizing Pod Autoscaling Cluster Autoscaling Leveraging Cloud Discounts","title":"Welcome to the Kubernetes Cost Optimization Guide"},{"location":"cloud-discounts/","text":"Leveraging Cloud Discounts Spot VMs and Best Effort Pods Combining Spot and On-Demand Fleets Applying Saving Plans to your Kubernetes clusters","title":"Leveraging Cloud Discounts"},{"location":"cloud-discounts/#leveraging-cloud-discounts","text":"Spot VMs and Best Effort Pods Combining Spot and On-Demand Fleets Applying Saving Plans to your Kubernetes clusters","title":"Leveraging Cloud Discounts"},{"location":"cluster-autoscaling/","text":"Cluster Autoscaling Cluster-autoscaler Karpenter Improving Kubernetes Bin Packing","title":"Cluster Autoscaling"},{"location":"cluster-autoscaling/#cluster-autoscaling","text":"Cluster-autoscaler Karpenter Improving Kubernetes Bin Packing","title":"Cluster Autoscaling"},{"location":"contact-us/","text":"Got a question? Found a mistake? Would like to contribute a guide? Let us know in this repository's discussions Or join our Slack community","title":"Contact Us"},{"location":"cost-perf-r9y/","text":"Balancing Cost with Performance and Reliability Kubernetes cost optimization comes down to pinpointing the correct resource allocations and auto-scaling factors for our workloads. But \"correct\" in this context doesn't mean \"the least possible amount of resources\". It's a delicate interplay of cost vs. performance vs.reliability. In order to run our clusters in the most cost-effective way without compromising either performance or reliability it's vitally inportant to understand the Pod QoS model and the implications of PodDisruptionBudget. Understanding the Pod QoS Model Kubernetes cost optimization starts with correct resource allocation for application containers that Kubernetes orchestrates. Each container can have requests and limits defined for either memory or cpu or both. PodDisruptionBudget and application disruption","title":"Balancing Cost with Performance and Reliability"},{"location":"cost-perf-r9y/#balancing-cost-with-performance-and-reliability","text":"Kubernetes cost optimization comes down to pinpointing the correct resource allocations and auto-scaling factors for our workloads. But \"correct\" in this context doesn't mean \"the least possible amount of resources\". It's a delicate interplay of cost vs. performance vs.reliability. In order to run our clusters in the most cost-effective way without compromising either performance or reliability it's vitally inportant to understand the Pod QoS model and the implications of PodDisruptionBudget.","title":"Balancing Cost with Performance and Reliability"},{"location":"cost-perf-r9y/#understanding-the-pod-qos-model","text":"Kubernetes cost optimization starts with correct resource allocation for application containers that Kubernetes orchestrates. Each container can have requests and limits defined for either memory or cpu or both. PodDisruptionBudget and application disruption","title":"Understanding the Pod QoS Model"},{"location":"golden-signals/","text":"The Golden Signals The 4 \"golden signals\" of Kubernetes Cost Optimization as defined in a whitepaper released by Google Cloud in June 2023. Signal Group 1.Workload Rightsizing Resources 2.Demand-based Downscaling 3.Cluster Bin Packing 4.Cloud Provider Discount Coverage Cloud Discounts These signals help us apply and measure cost optimization for Kubernets clusters. The 3 signals in the resources group apply to all clusters - be it on-prem or on-cloud. The cloud discounts naturally only apply to cloud-based managed clusters, where it is very important to pinpoint the instance types and reservation level of our cluster nodes. Let's explain each signal in a bit more detail. The Resources Group Signal Explanation Workload Rightsizing Refers to our ability to allocate the amount of resources that the workloads actually need and adapt resource requests and limits as application requirements change. Demand based autoscaling Measures the capacity of developers and platform admins to make clusters scale down during off-peak hours. Cluster bin packing Refers to our ability to measure and utilize the CPU and memory of each node in the most effective and reliable way through correct Pod placement. The Cloud Discounts group Signal Explanation Cloud Discount Coverage Refers to leveraging cloud VM instances that offer discounts, such as Spot VMs, as well as the ability of budget owners and FinOps professionals to take advantage of long-term continuous use discounts offered by cloud providers.","title":"The Golden Signals of Kubernetes Cost Optimization"},{"location":"golden-signals/#the-golden-signals","text":"The 4 \"golden signals\" of Kubernetes Cost Optimization as defined in a whitepaper released by Google Cloud in June 2023. Signal Group 1.Workload Rightsizing Resources 2.Demand-based Downscaling 3.Cluster Bin Packing 4.Cloud Provider Discount Coverage Cloud Discounts These signals help us apply and measure cost optimization for Kubernets clusters. The 3 signals in the resources group apply to all clusters - be it on-prem or on-cloud. The cloud discounts naturally only apply to cloud-based managed clusters, where it is very important to pinpoint the instance types and reservation level of our cluster nodes. Let's explain each signal in a bit more detail.","title":"The Golden Signals"},{"location":"golden-signals/#the-resources-group","text":"Signal Explanation Workload Rightsizing Refers to our ability to allocate the amount of resources that the workloads actually need and adapt resource requests and limits as application requirements change. Demand based autoscaling Measures the capacity of developers and platform admins to make clusters scale down during off-peak hours. Cluster bin packing Refers to our ability to measure and utilize the CPU and memory of each node in the most effective and reliable way through correct Pod placement.","title":"The Resources Group"},{"location":"golden-signals/#the-cloud-discounts-group","text":"Signal Explanation Cloud Discount Coverage Refers to leveraging cloud VM instances that offer discounts, such as Spot VMs, as well as the ability of budget owners and FinOps professionals to take advantage of long-term continuous use discounts offered by cloud providers.","title":"The Cloud Discounts group"},{"location":"hpa/","text":"HPA - The Horizontal Pod Autoscaler Horizontal Pod Autoscaler (HPA) in Kubernetes is a controller that automatically adjusts the number of pods based on observed CPU or memory utilization (default mode) or other metrics. The goal of HPA is to ensure that applications can handle varying loads efficiently by scaling out (increasing the number of pods) during high demand and scaling in (decreasing the number of pods) during low demand. This dynamic scaling helps maintain resource utilization and application performance. HPA continuously monitors the specified metrics and adjusts the replica count to match the desired state, ensuring that the application remains responsive. How HPA works HPA works as a control loop that runs at regular intervals. During each interval, the HPA controller queries the resource utilization metrics specified in the HPA configuration. The controller identifies the target resource defined by the scaleTargetRef field in the HPA configuration. It then selects the pods based on the target resource's selector labels and fetches the relevant metrics. For CPU and memory metrics, the controller uses the resource metrics API. For custom metrics, it uses the custom metrics API. If you want to create your own custom metrics adapter, take a look at the starter [template] (https://github.com/kubernetes-sigs/custom-metrics-apiserver) For metrics of individual pod resources, such as CPU usage, the HPA controller collects data for each pod that it targets. When a target utilization value is specified, the controller calculates the CPU utilization as a percentage of the resource request defined for each container within the pod. If any containers within a pod lack the necessary resource request settings, the CPU utilization for that pod will not be defined, and the autoscaler will not take any action based on the metric. HPA generally retrieves metrics from aggregated APIs like metrics.k8s.io , custom.metrics.k8s.io , or external.metrics.k8s.io . The metrics.k8s.io API is generally provided by an add-on called Metrics Server, which must be deployed separately. The Horizontal Pod Autoscaler (HPA) uses a straightforward algorithm to adjust the number of pod replicas based on the ratio of current metric values to desired metric values. The core formula is: desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )] For example, if the current CPU usage is 300m and the target is 150m, the number of replicas will double because 300.0 / 150.0 = 2.0 . Conversely, if the current usage is 75m, the replicas will be halved because 75.0 / 150.0 = 0.5 . Kubernetes HPA Shortcomings - Cost and Performance Metric Constraints and Conflicts : HPA cannot be used with VPA when both are based on CPU or memory metrics. This is because VPA adjusts resource requests and limits, which can conflict with HPA's scaling decisions. This limitation can lead to suboptimal resource allocation, impacting both cost efficiency and performance. Limited Resource Consideration : HPA doesn't consider IOPS, network bandwidth, or storage usage in its scaling decisions. This oversight can lead to performance bottlenecks or outages if these resources become constrained, potentially resulting in poor user experience and increased operational costs to address these issues reactively. Granularity and Resource Waste : HPA scales at the pod level, which may not provide the fine-grained control needed for certain applications. This can result in over-scaling or under-scaling, impacting both application performance and resource utilization. Moreover, HPA doesn't address resource waste within the Kubernetes cluster, leaving administrators responsible for identifying and managing unused or over-provisioned resources at the container level. This can lead to inefficiencies and increased costs if not carefully monitored. Scaling Latency and Stability : HPA operates on a polling interval (default 15 seconds), introducing a delay between detecting a need for scaling and implementing the change. This latency can be problematic during sudden traffic spikes, potentially leading to temporary performance degradation. To compensate, administrators might set more aggressive scaling parameters, potentially increasing resource costs. Moreover, without proper cooldown periods, HPA can cause rapid scaling up and down, leading to instability or \"flapping\". Careful configuration of stabilization windows is necessary to ensure stable scaling behavior, adding to management complexity. Resource Utilization Fluctuations : HPA makes scaling decisions based on current resource utilization metrics, which can be challenging for workloads with high variability. This can lead to \"thrashing,\" where the system rapidly scales up and down in response to short-term fluctuations. Thrashing impacts both performance stability and cost efficiency, as each scaling operation consumes resources and can incur additional costs, especially in cloud environments. Sampling rate limitations : Kubernetes' default configuration can hinder HPA's effectiveness for applications with rapidly changing workloads. The default metric collection interval of 15 seconds may not provide sufficiently granular data for making timely scaling decisions in highly dynamic environments. This limitation can result in delayed responses to changing conditions, potentially leading to performance issues during traffic spikes and inefficient resource utilization during lulls. This coarse sampling might lead to over-provisioning as a safety measure, increasing overall resource costs. Lack of Predictive Scaling : HPA's reactive approach, which responds to current conditions rather than anticipates future needs, can be suboptimal for applications with predictable traffic patterns or scheduled events. This can lead to temporary performance degradation during regular traffic spikes and impact cost efficiency, as resources are not preemptively allocated for known high-traffic periods. Cold Start Inefficiency : When new pods are created in response to increased demand, they require time to become fully operational. This includes time for container image pulling, application startup, and potentially warming up caches or establishing connections. During this period, the newly created pods consume resources but may not yet be able to handle their full share of the workload. This can lead to temporary resource inefficiency and potential performance impact during the scaling process. During a cold start, you're essentially paying for resources that are not yet fully utilized, which can impact overall cost efficiency, especially in environments with frequent scaling events. Optimizing HPA for Cost and Performance: Right-size your pods : Right-sizing your pods is a critical step in optimizing HPA for both cost and performance. By carefully configuring your pods with appropriate resource requests and limits, you create a more accurate baseline for HPA to make scaling decisions. This precision prevents unnecessary scaling events, which can lead to wasted resources and increased costs. Implement pod disruption budgets : Implementing Pod Disruption Budgets (PDBs) is an essential strategy for maintaining application stability during scaling events, which directly impacts both performance and cost efficiency. PDBs ensure that a minimum number of pods remain available during voluntary disruptions, such as node drains or cluster upgrades. This guarantee of minimum availability prevents performance degradation during scaling operations, ensuring that your application continues to serve requests effectively. PDBs help avoid scenarios where excessive pod terminations might trigger unnecessary scale-up events, thus preventing wasteful resource allocation and associated costs. An example of a PDB: apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: my-app-pdb spec: minAvailable: 2 selector: matchLabels: app: my-app Utilize custom metrics : Instead of relying solely on generic CPU and memory metrics, custom metrics allow you to scale based on application-specific indicators that truly reflect your workload's needs. This approach leads to more precise and effective scaling decisions, ensuring that you're allocating resources where they're most needed. By scaling based on metrics that directly correlate with your application's performance, you can maintain optimal user experience while avoiding overprovisioning helps in optimizing both performance and cost. Adjust HPA parameters : Adjusting HPA parameters, particularly scaleUpStabilizationWindowSeconds and scaleDownStabilizationWindowSeconds, is crucial for fine-tuning your autoscaling behavior. These parameters help reduce thrashing \u2013 rapid scaling up and down \u2013 which can negatively impact both performance and cost. By setting appropriate stabilization windows, you give your system time to stabilize after a scaling event before making new scaling decisions. This approach prevents unnecessary scaling operations, reducing the associated performance overhead and resource costs. behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 100 periodSeconds: 15 Implement cluster autoscaling : Implementing cluster autoscaling in conjunction with HPA provides a more holistic approach to resource management, optimizing both performance and cost at the cluster level. While HPA ensures that your application has the right number of pods, cluster autoscaling ensures that your Kubernetes cluster has enough nodes to accommodate these pods. This synergy prevents scenarios where HPA decisions are constrained by cluster capacity, ensuring that your application can scale as needed for optimal performance. Cluster autoscaling allows you to efficiently utilize cloud resources, scaling your infrastructure up or down based on actual demand, thus avoiding the costs associated with over-provisioned clusters. Consider alternative solutions : Considering alternative solutions like the Kubernetes Event-Driven Autoscaler (KEDA) or custom autoscaling solutions can provide more granular control and predictive scaling capabilities for applications with complex requirements. These advanced solutions can optimize performance by reacting more quickly to changes in workload or even anticipating them. The ability to scale more precisely and proactively can lead to significant savings by ensuring that resources are allocated only when truly needed and in the right quantities.","title":"Horizontal Pod Autoscaling (HPA)"},{"location":"hpa/#hpa-the-horizontal-pod-autoscaler","text":"Horizontal Pod Autoscaler (HPA) in Kubernetes is a controller that automatically adjusts the number of pods based on observed CPU or memory utilization (default mode) or other metrics. The goal of HPA is to ensure that applications can handle varying loads efficiently by scaling out (increasing the number of pods) during high demand and scaling in (decreasing the number of pods) during low demand. This dynamic scaling helps maintain resource utilization and application performance. HPA continuously monitors the specified metrics and adjusts the replica count to match the desired state, ensuring that the application remains responsive.","title":"HPA - The Horizontal Pod Autoscaler"},{"location":"hpa/#how-hpa-works","text":"HPA works as a control loop that runs at regular intervals. During each interval, the HPA controller queries the resource utilization metrics specified in the HPA configuration. The controller identifies the target resource defined by the scaleTargetRef field in the HPA configuration. It then selects the pods based on the target resource's selector labels and fetches the relevant metrics. For CPU and memory metrics, the controller uses the resource metrics API. For custom metrics, it uses the custom metrics API. If you want to create your own custom metrics adapter, take a look at the starter [template] (https://github.com/kubernetes-sigs/custom-metrics-apiserver) For metrics of individual pod resources, such as CPU usage, the HPA controller collects data for each pod that it targets. When a target utilization value is specified, the controller calculates the CPU utilization as a percentage of the resource request defined for each container within the pod. If any containers within a pod lack the necessary resource request settings, the CPU utilization for that pod will not be defined, and the autoscaler will not take any action based on the metric. HPA generally retrieves metrics from aggregated APIs like metrics.k8s.io , custom.metrics.k8s.io , or external.metrics.k8s.io . The metrics.k8s.io API is generally provided by an add-on called Metrics Server, which must be deployed separately. The Horizontal Pod Autoscaler (HPA) uses a straightforward algorithm to adjust the number of pod replicas based on the ratio of current metric values to desired metric values. The core formula is: desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )] For example, if the current CPU usage is 300m and the target is 150m, the number of replicas will double because 300.0 / 150.0 = 2.0 . Conversely, if the current usage is 75m, the replicas will be halved because 75.0 / 150.0 = 0.5 .","title":"How HPA works"},{"location":"hpa/#kubernetes-hpa-shortcomings-cost-and-performance","text":"Metric Constraints and Conflicts : HPA cannot be used with VPA when both are based on CPU or memory metrics. This is because VPA adjusts resource requests and limits, which can conflict with HPA's scaling decisions. This limitation can lead to suboptimal resource allocation, impacting both cost efficiency and performance. Limited Resource Consideration : HPA doesn't consider IOPS, network bandwidth, or storage usage in its scaling decisions. This oversight can lead to performance bottlenecks or outages if these resources become constrained, potentially resulting in poor user experience and increased operational costs to address these issues reactively. Granularity and Resource Waste : HPA scales at the pod level, which may not provide the fine-grained control needed for certain applications. This can result in over-scaling or under-scaling, impacting both application performance and resource utilization. Moreover, HPA doesn't address resource waste within the Kubernetes cluster, leaving administrators responsible for identifying and managing unused or over-provisioned resources at the container level. This can lead to inefficiencies and increased costs if not carefully monitored. Scaling Latency and Stability : HPA operates on a polling interval (default 15 seconds), introducing a delay between detecting a need for scaling and implementing the change. This latency can be problematic during sudden traffic spikes, potentially leading to temporary performance degradation. To compensate, administrators might set more aggressive scaling parameters, potentially increasing resource costs. Moreover, without proper cooldown periods, HPA can cause rapid scaling up and down, leading to instability or \"flapping\". Careful configuration of stabilization windows is necessary to ensure stable scaling behavior, adding to management complexity. Resource Utilization Fluctuations : HPA makes scaling decisions based on current resource utilization metrics, which can be challenging for workloads with high variability. This can lead to \"thrashing,\" where the system rapidly scales up and down in response to short-term fluctuations. Thrashing impacts both performance stability and cost efficiency, as each scaling operation consumes resources and can incur additional costs, especially in cloud environments. Sampling rate limitations : Kubernetes' default configuration can hinder HPA's effectiveness for applications with rapidly changing workloads. The default metric collection interval of 15 seconds may not provide sufficiently granular data for making timely scaling decisions in highly dynamic environments. This limitation can result in delayed responses to changing conditions, potentially leading to performance issues during traffic spikes and inefficient resource utilization during lulls. This coarse sampling might lead to over-provisioning as a safety measure, increasing overall resource costs. Lack of Predictive Scaling : HPA's reactive approach, which responds to current conditions rather than anticipates future needs, can be suboptimal for applications with predictable traffic patterns or scheduled events. This can lead to temporary performance degradation during regular traffic spikes and impact cost efficiency, as resources are not preemptively allocated for known high-traffic periods. Cold Start Inefficiency : When new pods are created in response to increased demand, they require time to become fully operational. This includes time for container image pulling, application startup, and potentially warming up caches or establishing connections. During this period, the newly created pods consume resources but may not yet be able to handle their full share of the workload. This can lead to temporary resource inefficiency and potential performance impact during the scaling process. During a cold start, you're essentially paying for resources that are not yet fully utilized, which can impact overall cost efficiency, especially in environments with frequent scaling events.","title":"Kubernetes HPA Shortcomings - Cost and Performance"},{"location":"hpa/#optimizing-hpa-for-cost-and-performance","text":"Right-size your pods : Right-sizing your pods is a critical step in optimizing HPA for both cost and performance. By carefully configuring your pods with appropriate resource requests and limits, you create a more accurate baseline for HPA to make scaling decisions. This precision prevents unnecessary scaling events, which can lead to wasted resources and increased costs. Implement pod disruption budgets : Implementing Pod Disruption Budgets (PDBs) is an essential strategy for maintaining application stability during scaling events, which directly impacts both performance and cost efficiency. PDBs ensure that a minimum number of pods remain available during voluntary disruptions, such as node drains or cluster upgrades. This guarantee of minimum availability prevents performance degradation during scaling operations, ensuring that your application continues to serve requests effectively. PDBs help avoid scenarios where excessive pod terminations might trigger unnecessary scale-up events, thus preventing wasteful resource allocation and associated costs. An example of a PDB: apiVersion: policy/v1beta1 kind: PodDisruptionBudget metadata: name: my-app-pdb spec: minAvailable: 2 selector: matchLabels: app: my-app Utilize custom metrics : Instead of relying solely on generic CPU and memory metrics, custom metrics allow you to scale based on application-specific indicators that truly reflect your workload's needs. This approach leads to more precise and effective scaling decisions, ensuring that you're allocating resources where they're most needed. By scaling based on metrics that directly correlate with your application's performance, you can maintain optimal user experience while avoiding overprovisioning helps in optimizing both performance and cost. Adjust HPA parameters : Adjusting HPA parameters, particularly scaleUpStabilizationWindowSeconds and scaleDownStabilizationWindowSeconds, is crucial for fine-tuning your autoscaling behavior. These parameters help reduce thrashing \u2013 rapid scaling up and down \u2013 which can negatively impact both performance and cost. By setting appropriate stabilization windows, you give your system time to stabilize after a scaling event before making new scaling decisions. This approach prevents unnecessary scaling operations, reducing the associated performance overhead and resource costs. behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 100 periodSeconds: 15 Implement cluster autoscaling : Implementing cluster autoscaling in conjunction with HPA provides a more holistic approach to resource management, optimizing both performance and cost at the cluster level. While HPA ensures that your application has the right number of pods, cluster autoscaling ensures that your Kubernetes cluster has enough nodes to accommodate these pods. This synergy prevents scenarios where HPA decisions are constrained by cluster capacity, ensuring that your application can scale as needed for optimal performance. Cluster autoscaling allows you to efficiently utilize cloud resources, scaling your infrastructure up or down based on actual demand, thus avoiding the costs associated with over-provisioned clusters. Consider alternative solutions : Considering alternative solutions like the Kubernetes Event-Driven Autoscaler (KEDA) or custom autoscaling solutions can provide more granular control and predictive scaling capabilities for applications with complex requirements. These advanced solutions can optimize performance by reacting more quickly to changes in workload or even anticipating them. The ability to scale more precisely and proactively can lead to significant savings by ensuring that resources are allocated only when truly needed and in the right quantities.","title":"Optimizing HPA for Cost and Performance:"},{"location":"over-under-idle-waste/","text":"The 4 Focus Areas When starting out with Kubernetes Cost Optimization it's important to understand what to focus on. Redundant costs come from 2 main sources: wasted resources and idle resources . Both of these are usually caused by over-provisioning , intentional or unintentional. On the other hand - thoughtless cost reduction activity can lead to under-provisioning , which causes performance and reliability issues. When optimizing our cluster costs we want to focus on all of these areas iteratively - in order to keep our clusters as cost-effective and performant as needed. Now let's explain each of these focus areas in more detail. Wasted Resources Wasted resources are the resources that have been allocated but not utilized. In most unoptimizaed clusters we're observing up to 50% of waste, which translates to thousands of dollars or euros monthly. This waste comes from over-provisioning the containers in our pods. Read on to understand the reasons and perils of over-provsioning. Idle Resources Kubernetes comes with a promise of automatic bin packing. I.e - it is supposed to fit the largest possible amount of pods on every node in the cluster. But this is again dependent on engineers correctly defining 1) resource requests and 2) node sizes. Even with smart and well-tuned autoscaling tools like Karpenter this doesn't always work and we find ourselves with nodes that are more than half empty - with resources that were neither requested nor utilized. All these are idle resources and taking care of reducing them is an important focus area of Kubernetes Cost Optimization. Overprovisioning Pinpointing the exact memory and CPU requests for our pods is hard - it requires observing the application behaviour under production load over a significant time period. Therefore most engineers prefer to err towards overprovisioning - i.e setting requests much higher than the application will ever use. This leads to a large amount of allocated but unutilized resources all across the cluster. Just imagine your cluster runs 200 pods and each of them requests 100Mb more memory than it actually uses. Altogether you'll have 20Gb of wasted RAM across the cluster. These resources will be provisioned, paid for, but never actually used. Underprovisioning Container underprovisioning in Kubernetes occurs when the resources allocated to containers are insufficient to meet the demands of the applications they run. This scenario can arise from: underestimating the resource needs of an application excessively aggressive optimization based on incomplete data not setting resource requests altogether - which gives us BestEffort QoS pods Underprovisioning can lead to several issues, including poor application performance, increased latency, and even outages as containers are killed on OOM or evicted because of resource exhaustion. This can be especially problematic in production environments where reliability and responsiveness are critical. Browse the rest of our guides to learm how to address each one of these focus areas in your Kubernetes cost optimization effort.","title":"The 4 Focus Areas of Kubernetes Cost Optimization"},{"location":"over-under-idle-waste/#the-4-focus-areas","text":"When starting out with Kubernetes Cost Optimization it's important to understand what to focus on. Redundant costs come from 2 main sources: wasted resources and idle resources . Both of these are usually caused by over-provisioning , intentional or unintentional. On the other hand - thoughtless cost reduction activity can lead to under-provisioning , which causes performance and reliability issues. When optimizing our cluster costs we want to focus on all of these areas iteratively - in order to keep our clusters as cost-effective and performant as needed. Now let's explain each of these focus areas in more detail.","title":"The 4 Focus Areas"},{"location":"over-under-idle-waste/#wasted-resources","text":"Wasted resources are the resources that have been allocated but not utilized. In most unoptimizaed clusters we're observing up to 50% of waste, which translates to thousands of dollars or euros monthly. This waste comes from over-provisioning the containers in our pods. Read on to understand the reasons and perils of over-provsioning.","title":"Wasted Resources"},{"location":"over-under-idle-waste/#idle-resources","text":"Kubernetes comes with a promise of automatic bin packing. I.e - it is supposed to fit the largest possible amount of pods on every node in the cluster. But this is again dependent on engineers correctly defining 1) resource requests and 2) node sizes. Even with smart and well-tuned autoscaling tools like Karpenter this doesn't always work and we find ourselves with nodes that are more than half empty - with resources that were neither requested nor utilized. All these are idle resources and taking care of reducing them is an important focus area of Kubernetes Cost Optimization.","title":"Idle Resources"},{"location":"over-under-idle-waste/#overprovisioning","text":"Pinpointing the exact memory and CPU requests for our pods is hard - it requires observing the application behaviour under production load over a significant time period. Therefore most engineers prefer to err towards overprovisioning - i.e setting requests much higher than the application will ever use. This leads to a large amount of allocated but unutilized resources all across the cluster. Just imagine your cluster runs 200 pods and each of them requests 100Mb more memory than it actually uses. Altogether you'll have 20Gb of wasted RAM across the cluster. These resources will be provisioned, paid for, but never actually used.","title":"Overprovisioning"},{"location":"over-under-idle-waste/#underprovisioning","text":"Container underprovisioning in Kubernetes occurs when the resources allocated to containers are insufficient to meet the demands of the applications they run. This scenario can arise from: underestimating the resource needs of an application excessively aggressive optimization based on incomplete data not setting resource requests altogether - which gives us BestEffort QoS pods Underprovisioning can lead to several issues, including poor application performance, increased latency, and even outages as containers are killed on OOM or evicted because of resource exhaustion. This can be especially problematic in production environments where reliability and responsiveness are critical. Browse the rest of our guides to learm how to address each one of these focus areas in your Kubernetes cost optimization effort.","title":"Underprovisioning"},{"location":"pod-autoscaling/","text":"Pod Autoscaling Autoscaling is a widely accepted practice in cloud computing which entails automatically adjusting the amount of computational resources based on load. Autoscaling automates resource management and as such has a significant impact on cluster costs. Well configured autoscaling can make your cluster lean, efficient and reliable. Badly configured autoscaling can generate waste and reduce availability. Kubernetes comes with add-ons that allow us to autoscale pods in 2 distinct ways: Horizontally By changing the number of pod replicas in a workload (Deployment, StatefulSet) Vertically By changing the amount of resources (CPU, memory) allocated to each individual pod in a workload. Horizontal Pod Autoscaling Horizontal pod autoscaling is enabled by the following add-ons: (Follow the links for autoscaling optimization recommendations) HPA KEDA Vertical Pod Autoscaling Horizontal pod autosscaling is enabled by the following add-ons: (Follow the links for autoscaling optimization recommendations) VPA Goldilocks","title":"Overview"},{"location":"pod-autoscaling/#pod-autoscaling","text":"Autoscaling is a widely accepted practice in cloud computing which entails automatically adjusting the amount of computational resources based on load. Autoscaling automates resource management and as such has a significant impact on cluster costs. Well configured autoscaling can make your cluster lean, efficient and reliable. Badly configured autoscaling can generate waste and reduce availability. Kubernetes comes with add-ons that allow us to autoscale pods in 2 distinct ways: Horizontally By changing the number of pod replicas in a workload (Deployment, StatefulSet) Vertically By changing the amount of resources (CPU, memory) allocated to each individual pod in a workload.","title":"Pod Autoscaling"},{"location":"pod-autoscaling/#horizontal-pod-autoscaling","text":"Horizontal pod autoscaling is enabled by the following add-ons: (Follow the links for autoscaling optimization recommendations) HPA KEDA","title":"Horizontal Pod Autoscaling"},{"location":"pod-autoscaling/#vertical-pod-autoscaling","text":"Horizontal pod autosscaling is enabled by the following add-ons: (Follow the links for autoscaling optimization recommendations) VPA Goldilocks","title":"Vertical Pod Autoscaling"},{"location":"resources/","text":"The Economics of Kubernetes Resource Allocation This whole guide talks a lot about resources which is a highly overloaded word in Kubernetes world. All the objects defined in Kubernetes API (such as Pod, Service, ConfigMap, etc) are also called resources . But we're not referring to them here. So in order to make things clearer let's define resources for the purpose of this guide. When we say resources - we actually mean CPU, GPU, memory, network and storage. In the pre-cloud world all these needed to be defined in advance. Ordering and provisioning these resources took weeks or even months. In cloud native environments (i.e Kubernetes) these resources are highly dynamic in nature and can be allocated and released on demand - just by issuing an API call. The process of allocating additional resources in such an automated manner is called autoscaling. This automation gives us a lot of power by prividing access to addtional resources when needed (aka just-in-time provisioning). And it also creates undesirable artifacts if not configured correctly such as: Wasted resources (when we allocate more than we actually need) Reliability issues (when provisioning doesn't work as expected) Unexpected costs (when we don't have good control over what and when gets provisioned) Resource Allocation in Kubernetes Kubernetes has different ways of allocating, limiting and provisioning resources for application containers. CPU and memory allocation On the very basic level - engineers can request CPU and memory for a container by defining its resource requests and limits in the Pod resource spec: apiVersion: v1 kind: Pod metadata: name: example spec: containers: - image: perfectscale.io/example name: example resources: requests: cpu: 1 memory: 500Mi limits: cpu: 1 memory: 500Mi Storage Resource Allocation In order to allocate storage Kubernetes allows us to use its PersistentVolume allocation mechanisms in conjunction with one of the multiple supported storage providers (e.g OpenEBS, Portworx, etc.) Network Resource Allocation Network resources aren't managed by Kubernetes itself but instead are delegated to one of the multiple CNI networking providers, which are reponsible for provisioning, allocating and retiring network interfaces and addresses. Continue here to get a better understanding of the 4 main focus areas of Kubernetes Cost Optimization .","title":"The Economics of Kubernetes Resource Allocation"},{"location":"resources/#the-economics-of-kubernetes-resource-allocation","text":"This whole guide talks a lot about resources which is a highly overloaded word in Kubernetes world. All the objects defined in Kubernetes API (such as Pod, Service, ConfigMap, etc) are also called resources . But we're not referring to them here. So in order to make things clearer let's define resources for the purpose of this guide. When we say resources - we actually mean CPU, GPU, memory, network and storage. In the pre-cloud world all these needed to be defined in advance. Ordering and provisioning these resources took weeks or even months. In cloud native environments (i.e Kubernetes) these resources are highly dynamic in nature and can be allocated and released on demand - just by issuing an API call. The process of allocating additional resources in such an automated manner is called autoscaling. This automation gives us a lot of power by prividing access to addtional resources when needed (aka just-in-time provisioning). And it also creates undesirable artifacts if not configured correctly such as: Wasted resources (when we allocate more than we actually need) Reliability issues (when provisioning doesn't work as expected) Unexpected costs (when we don't have good control over what and when gets provisioned)","title":"The Economics of Kubernetes Resource Allocation"},{"location":"resources/#resource-allocation-in-kubernetes","text":"Kubernetes has different ways of allocating, limiting and provisioning resources for application containers.","title":"Resource Allocation in Kubernetes"},{"location":"resources/#cpu-and-memory-allocation","text":"On the very basic level - engineers can request CPU and memory for a container by defining its resource requests and limits in the Pod resource spec: apiVersion: v1 kind: Pod metadata: name: example spec: containers: - image: perfectscale.io/example name: example resources: requests: cpu: 1 memory: 500Mi limits: cpu: 1 memory: 500Mi","title":"CPU and memory allocation"},{"location":"resources/#storage-resource-allocation","text":"In order to allocate storage Kubernetes allows us to use its PersistentVolume allocation mechanisms in conjunction with one of the multiple supported storage providers (e.g OpenEBS, Portworx, etc.)","title":"Storage Resource Allocation"},{"location":"resources/#network-resource-allocation","text":"Network resources aren't managed by Kubernetes itself but instead are delegated to one of the multiple CNI networking providers, which are reponsible for provisioning, allocating and retiring network interfaces and addresses. Continue here to get a better understanding of the 4 main focus areas of Kubernetes Cost Optimization .","title":"Network Resource Allocation"},{"location":"rightsizing/","text":"Kubernetes Workload Rightsizing Requests Memory CPU Limits Memory CPU Understanding CPU throttling Defining resource guardrails LimitRange NamespaceQuota","title":"Kubernetes Workload Rightsizing"},{"location":"rightsizing/#kubernetes-workload-rightsizing","text":"Requests Memory CPU Limits Memory CPU Understanding CPU throttling Defining resource guardrails LimitRange NamespaceQuota","title":"Kubernetes Workload Rightsizing"},{"location":"vpa/","text":"What's VPA and How It Works Vertical Pod Autoscaler (VPA) is Kubernetes' implementation of vertical autoscaling. VPA is designed to automatically adjust the CPU and memory resource requests and limits for containers within pods. VPA aims to solve one of the most challenging aspects of Kubernetes resource management: accurately setting resource requests and limits. VPA continuously monitors the resource usage of your pods and makes recommendations or automatic adjustments to ensure your containers have the right amount of resources. Using VPA in a correct way offers multiple benefits that directly impact resource and cost efficiency. By dynamically adjusting resource allocations based on actual usage, it ensures efficient resource utilization and reduces overprovisioning, a common pitfall in manual resource management. This leads to an improved performance-to-cost ratio, as more workloads can run on the same infrastructure. The automatic nature of these adjustments saves valuable time and effort that would otherwise be spent on manual tuning, allowing teams to focus on more strategic tasks. As applications' resource needs evolve over time, vertical autoscaling adapts accordingly, maintaining optimal resource allocation without human intervention. This adaptability, combined with more efficient resource use, contributes to better cost predictability and an overall reduction in cloud spending. Ultimately, vertical autoscaling creates a more cost-effective Kubernetes environment that automatically balances performance needs with resource efficiency, optimizing cloud spending while maintaining system reliability and performance. VPA consists of 3 main components (each running as a separate deployment): the VPA Recommender the VPA Updater the VPA Admission Controller These components work together to collect data, analyze resource usage, generate recommendations, and apply changes to pod specifications. The VPA Recommender is responsible for analyzing resource usage patterns and generating recommendations for CPU and memory settings. It continuously monitors the resource consumption of containers using metrics provided by the Kubernetes Metrics Server. The VPA Updater is the component responsible for applying the recommendations generated by the Recommender. When operating in \"Auto\" mode, the Updater will delete pods that need updating and create new ones with the adjusted resource settings. It's important to note that this process can be affected by Pod Disruption Budget (PDB) settings, stalling the update process. In the future - when the InPlacePodVerticalScaling feature goes out of Alpha - pod updates should be possible without deletion The VPA Admission Controller intercepts pod creation requests and modifies the resource requirements according to the VPA recommendations. This ensures that even newly created pods start with adjusted resource settings, improving overall cluster efficiency from the outset. One of the key features of VPA is its ability to operate in different modes, providing flexibility to cluster administrators. In \"Off\" mode, VPA generates recommendations but does not apply them automatically, allowing for manual review and application. The \"Initial\" mode applies recommendations only when new pods are created, which can be useful in scenarios where pod restarts are not desirable. The \"Auto\" mode, as mentioned earlier, actively applies recommendations to both new and existing pods. VPA incorporates several safety measures to prevent potential issues that could arise from frequent resource adjustments. It respects pod disruption budgets to ensure service availability, implements hysteresis in its decision-making process to avoid oscillations, and can be configured with min/max boundaries to prevent extreme resource allocations. VPA Limitations 1. VPA focuses on pod resource usage without considering the available node resources. This can lead to recommendations that, while optimal for the pod, might not be feasible given the cluster's actual capacity, resulting in pods that can't be scheduled. 2. Java applications with their complex memory management through the JVM present a unique challenge. VPA may struggle to accurately gauge the true resource needs of these applications, leading to suboptimal scaling decisions. Also, it can't identify memory leaks and JVM CPU init bursts. 3. To implement resource changes, VPA needs to recreate pods. This process, while necessary for applying new configurations, can cause brief periods of unavailability for the affected workloads, which might be problematic for applications requiring high availability. 4. While VPA works well in smaller environments, its performance in large, production-scale clusters with hundreds or thousands of nodes and pods remains a question mark. This uncertainty can be a significant concern for enterprises considering VPA for their large-scale deployments. 5. By focusing primarily on CPU and memory, VPA overlooks other crucial resources like network bandwidth and disk I/O. In I/O-intensive applications, this oversight can lead to performance bottlenecks that VPA won't address or may even exacerbate. 6. For stateful applications, the pod restart process during VPA updates can be more disruptive and may require additional considerations, such as proper handling of data consistency and state management during restarts. 7. The VPA operates based on historical resource usage data without considering workload revisions or updates. In modern, fast-paced environments where new versions of applications are frequently deployed, this can lead to suboptimal resource recommendations. VPA may apply the same resource adjustments to a newly deployed version that has different resource requirements than its predecessor. This limitation can result in unnecessary pod mutations and inappropriate resource allocations for new revisions, especially in environments with daily or frequent deployments. Optimizing VPA: Let's dive into strategies for optimizing VPA\u2019s use to maximize cost savings and efficiency: 1. Start with Proper Resource Benchmarking: Before implementing VPA, conduct thorough resource benchmarking of your applications. This will give you a baseline understanding of resource usage patterns and help you set initial requests more accurately preventing over-provisioning from the start, immediately reducing costs. You can use tools like Prometheus and Grafana for this process. 2. Use the Right Update Mode: Start with \"Initial\" mode, then move to \"Auto\" gradually. This approach allows you to monitor cost impacts and fine-tune before fully automating, preventing unexpected spikes in resource allocation and costs. 3. Set Appropriate Resource Bounds: Use the `minAllowed` and `maxAllowed` fields in your VPA configuration to set lower and upper bounds for resource requests. This prevents over-allocation while still allowing for necessary scaling, directly impacting cost efficiency. VPA configuration with resource bounds: apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: my-app-vpa spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: my-app updatePolicy: updateMode: \"Auto\" resourcePolicy: containerPolicies: - containerName: '\\*' minAllowed: cpu: 100m memory: 128Mi maxAllowed: cpu: 1 memory: 1Gi 4. Use Pod Disruption Budgets: Use Pod Disruption Budgets (PDBs) in conjunction with VPA, but configure them with cost in mind. Allow for more disruption during off-peak hours when it's cheaper to reallocate resources: apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: cost-aware-pdb spec: maxUnavailable: 50% selector: matchLabels: app: my-app 6. Use VPA Recommender Histograms: Regularly analyze VPA histogram data to identify cost-saving opportunities. Look for patterns where resources are under-utilized and adjust your VPA configuration accordingly. 7. Implement Custom Resource Policies: Use custom resource policies to fine-tune how VPA handles specific containers or sets of containers. This allows for more granular control over resource management. By tailoring resource policies to specific workload requirements, you can optimize resource allocation and reduce costs by preventing over-provisioning of resources for containers that don't need them. 8. Fine-Tuning with Multiple Recommenders: One advanced technique for optimizing VPA is to leverage multiple recommenders. While Kubernetes' default VPA implementation uses a single recommender, it's possible to implement and integrate custom recommenders to achieve more nuanced resource allocation, leading to improved cost efficiency and optimal performance. Why Use Multiple Recommenders? Utilizing multiple recommenders in your resource optimization strategy offers several advantages for cost and resource efficiency. By implementing specialized algorithms tailored to different workload types, you can achieve more accurate and context-aware resource recommendations, preventing over-provisioning and reducing costs. This approach allows you to incorporate business-specific logic, such as cost thresholds or performance SLAs, directly into the recommendation process. Custom recommenders enable the integration of external data sources, providing valuable insights from anticipated traffic patterns or scheduled batch jobs. Using recommenders that operate on various time scales allows for both short-term adjustments and long-term capacity planning. This multi-faceted approach can lead to more efficient resource utilization, reducing costs while maintaining the application performance. Also, custom recommenders can be designed to consider factors like node affinity, pod disruption budgets, or specific application behaviors that may not be captured by default recommenders. By using multiple recommenders, you create a more robust and adaptive system. Implementing Multiple Recommenders: This approach involves creating custom recommenders using the Kubernetes Metrics API and VPA APIs, implemented as separate microservices within your cluster. A crucial component in this setup is the Recommendation Aggregator, which combines inputs from various recommenders and makes final decisions based on predefined rules or machine learning models. To integrate this system with the VPA, you'll need to modify the VPA controller to use your custom aggregator instead of the default recommender, which requires changes to the VPA source code and rebuilding its components. Let\u2019s take an example of an e-commerce application, you might have one recommender analyzing historical sales data to predict resource needs for upcoming events, another monitoring real-time user traffic for immediate adjustments, and a third incorporating cost data to ensure recommendations stay within budget constraints. The aggregator would then synthesize these inputs to provide a balanced recommendation that optimizes both performance and cost. It's important to thoroughly test this custom setup in a non-production environment before deployment. This approach can lead to more accurate, context-aware resource optimization, making improvements in both cost efficiency and application performance. You can have a look here for more information. 6. Integrating VPA with Cluster Autoscaler or Karpenter: To achieve true cost optimization, it's crucial to integrate VPA with cluster-level autoscaling solutions like Cluster Autoscaler or Karpenter. This integration ensures that not only are your pods right-sized, but your overall cluster resources are also optimized. Why is this integration important? VPA focuses on optimizing resources at the pod level, but it doesn't directly influence the number or size of nodes in your cluster. Without proper cluster-level autoscaling, you might end up with right-sized pods but underutilized or overloaded nodes, which doesn't translate to real cost savings. Integrating with Cluster Autoscaler: Integrating VPA with Cluster Autoscaler requires an understanding of their interplay and configuration. Cluster Autoscaler responds to pod scheduling needs by adding or removing nodes, which can be indirectly triggered by VPA's resource adjustments. To optimize this integration, ensure your Cluster Autoscaler is configured with appropriate min/max node counts and scale-down delays to prevent unnecessary scaling actions that could lead to increased costs. Implement Kubernetes Pod Priority and Preemption to ensure critical pods are scheduled even during cluster scaling events. Closely monitor the interaction between VPA and Cluster Autoscaler, looking for patterns where VPA adjustments consistently trigger scaling events, and adjust configurations to minimize unnecessary infrastructure costs. This integrated approach will lead to a more cost-effective Kubernetes environment with optimized resource utilization. Integrating with Karpenter: Karpenter is a more flexible and efficient cluster autoscaler for Kubernetes. Integrating VPA with Karpenter can lead to even more optimized resource utilization and cost savings: 1. Karpenter's Fast Node Provisioning: Karpenter can quickly provision nodes that precisely match pod requirements, reducing overprovisioning and associated costs. This works well with VPA's dynamic resource adjustments to ensure optimal resource allocation. 2. Use Karpenter Provisioners: Configure Karpenter Provisioners to align with your VPA strategies and cost objectives. Create provisioners optimized for specific workload types that VPA is managing, ensuring the most cost-effective instance types are used. 3. Implement Consolidation: Take advantage of Karpenter's consolidation feature, which can move pods to optimize node utilization. This complements VPA's pod-level optimizations, further reducing infrastructure costs by eliminating underutilized nodes. 4. Custom Resource Management: Utilize Karpenter's support for custom resources in conjunction with VPA to manage specialized workloads more effectively, ensuring that expensive custom resources are allocated efficiently. To effectively integrate VPA and cluster autoscaling, adopt a gradual implementation approach, starting with separate implementations and progressively integrating them for non-critical workloads. Set conservative thresholds initially to prevent rapid fluctuations, adjusting as system stability and cost benefits are confirmed. Implement monitoring using tools like Prometheus and Grafana to track both pod-level and node-level metrics. Utilize Kubernetes simulation tools to model the impact of VPA and autoscaling decisions before production deployment. Design your system to handle potential failure scenarios, incorporating manual override procedures and fallback configurations to maintain system resilience and cost efficiency.","title":"Vertical Pod Autoscaling (VPA)"},{"location":"vpa/#whats-vpa-and-how-it-works","text":"Vertical Pod Autoscaler (VPA) is Kubernetes' implementation of vertical autoscaling. VPA is designed to automatically adjust the CPU and memory resource requests and limits for containers within pods. VPA aims to solve one of the most challenging aspects of Kubernetes resource management: accurately setting resource requests and limits. VPA continuously monitors the resource usage of your pods and makes recommendations or automatic adjustments to ensure your containers have the right amount of resources. Using VPA in a correct way offers multiple benefits that directly impact resource and cost efficiency. By dynamically adjusting resource allocations based on actual usage, it ensures efficient resource utilization and reduces overprovisioning, a common pitfall in manual resource management. This leads to an improved performance-to-cost ratio, as more workloads can run on the same infrastructure. The automatic nature of these adjustments saves valuable time and effort that would otherwise be spent on manual tuning, allowing teams to focus on more strategic tasks. As applications' resource needs evolve over time, vertical autoscaling adapts accordingly, maintaining optimal resource allocation without human intervention. This adaptability, combined with more efficient resource use, contributes to better cost predictability and an overall reduction in cloud spending. Ultimately, vertical autoscaling creates a more cost-effective Kubernetes environment that automatically balances performance needs with resource efficiency, optimizing cloud spending while maintaining system reliability and performance. VPA consists of 3 main components (each running as a separate deployment): the VPA Recommender the VPA Updater the VPA Admission Controller These components work together to collect data, analyze resource usage, generate recommendations, and apply changes to pod specifications. The VPA Recommender is responsible for analyzing resource usage patterns and generating recommendations for CPU and memory settings. It continuously monitors the resource consumption of containers using metrics provided by the Kubernetes Metrics Server. The VPA Updater is the component responsible for applying the recommendations generated by the Recommender. When operating in \"Auto\" mode, the Updater will delete pods that need updating and create new ones with the adjusted resource settings. It's important to note that this process can be affected by Pod Disruption Budget (PDB) settings, stalling the update process. In the future - when the InPlacePodVerticalScaling feature goes out of Alpha - pod updates should be possible without deletion The VPA Admission Controller intercepts pod creation requests and modifies the resource requirements according to the VPA recommendations. This ensures that even newly created pods start with adjusted resource settings, improving overall cluster efficiency from the outset. One of the key features of VPA is its ability to operate in different modes, providing flexibility to cluster administrators. In \"Off\" mode, VPA generates recommendations but does not apply them automatically, allowing for manual review and application. The \"Initial\" mode applies recommendations only when new pods are created, which can be useful in scenarios where pod restarts are not desirable. The \"Auto\" mode, as mentioned earlier, actively applies recommendations to both new and existing pods. VPA incorporates several safety measures to prevent potential issues that could arise from frequent resource adjustments. It respects pod disruption budgets to ensure service availability, implements hysteresis in its decision-making process to avoid oscillations, and can be configured with min/max boundaries to prevent extreme resource allocations.","title":"What's VPA and How It Works"},{"location":"vpa/#vpa-limitations","text":"1. VPA focuses on pod resource usage without considering the available node resources. This can lead to recommendations that, while optimal for the pod, might not be feasible given the cluster's actual capacity, resulting in pods that can't be scheduled. 2. Java applications with their complex memory management through the JVM present a unique challenge. VPA may struggle to accurately gauge the true resource needs of these applications, leading to suboptimal scaling decisions. Also, it can't identify memory leaks and JVM CPU init bursts. 3. To implement resource changes, VPA needs to recreate pods. This process, while necessary for applying new configurations, can cause brief periods of unavailability for the affected workloads, which might be problematic for applications requiring high availability. 4. While VPA works well in smaller environments, its performance in large, production-scale clusters with hundreds or thousands of nodes and pods remains a question mark. This uncertainty can be a significant concern for enterprises considering VPA for their large-scale deployments. 5. By focusing primarily on CPU and memory, VPA overlooks other crucial resources like network bandwidth and disk I/O. In I/O-intensive applications, this oversight can lead to performance bottlenecks that VPA won't address or may even exacerbate. 6. For stateful applications, the pod restart process during VPA updates can be more disruptive and may require additional considerations, such as proper handling of data consistency and state management during restarts. 7. The VPA operates based on historical resource usage data without considering workload revisions or updates. In modern, fast-paced environments where new versions of applications are frequently deployed, this can lead to suboptimal resource recommendations. VPA may apply the same resource adjustments to a newly deployed version that has different resource requirements than its predecessor. This limitation can result in unnecessary pod mutations and inappropriate resource allocations for new revisions, especially in environments with daily or frequent deployments.","title":"VPA Limitations"},{"location":"vpa/#optimizing-vpa","text":"Let's dive into strategies for optimizing VPA\u2019s use to maximize cost savings and efficiency: 1. Start with Proper Resource Benchmarking: Before implementing VPA, conduct thorough resource benchmarking of your applications. This will give you a baseline understanding of resource usage patterns and help you set initial requests more accurately preventing over-provisioning from the start, immediately reducing costs. You can use tools like Prometheus and Grafana for this process. 2. Use the Right Update Mode: Start with \"Initial\" mode, then move to \"Auto\" gradually. This approach allows you to monitor cost impacts and fine-tune before fully automating, preventing unexpected spikes in resource allocation and costs. 3. Set Appropriate Resource Bounds: Use the `minAllowed` and `maxAllowed` fields in your VPA configuration to set lower and upper bounds for resource requests. This prevents over-allocation while still allowing for necessary scaling, directly impacting cost efficiency. VPA configuration with resource bounds: apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: my-app-vpa spec: targetRef: apiVersion: \"apps/v1\" kind: Deployment name: my-app updatePolicy: updateMode: \"Auto\" resourcePolicy: containerPolicies: - containerName: '\\*' minAllowed: cpu: 100m memory: 128Mi maxAllowed: cpu: 1 memory: 1Gi 4. Use Pod Disruption Budgets: Use Pod Disruption Budgets (PDBs) in conjunction with VPA, but configure them with cost in mind. Allow for more disruption during off-peak hours when it's cheaper to reallocate resources: apiVersion: policy/v1 kind: PodDisruptionBudget metadata: name: cost-aware-pdb spec: maxUnavailable: 50% selector: matchLabels: app: my-app 6. Use VPA Recommender Histograms: Regularly analyze VPA histogram data to identify cost-saving opportunities. Look for patterns where resources are under-utilized and adjust your VPA configuration accordingly. 7. Implement Custom Resource Policies: Use custom resource policies to fine-tune how VPA handles specific containers or sets of containers. This allows for more granular control over resource management. By tailoring resource policies to specific workload requirements, you can optimize resource allocation and reduce costs by preventing over-provisioning of resources for containers that don't need them. 8. Fine-Tuning with Multiple Recommenders: One advanced technique for optimizing VPA is to leverage multiple recommenders. While Kubernetes' default VPA implementation uses a single recommender, it's possible to implement and integrate custom recommenders to achieve more nuanced resource allocation, leading to improved cost efficiency and optimal performance.","title":"Optimizing VPA:"},{"location":"vpa/#why-use-multiple-recommenders","text":"Utilizing multiple recommenders in your resource optimization strategy offers several advantages for cost and resource efficiency. By implementing specialized algorithms tailored to different workload types, you can achieve more accurate and context-aware resource recommendations, preventing over-provisioning and reducing costs. This approach allows you to incorporate business-specific logic, such as cost thresholds or performance SLAs, directly into the recommendation process. Custom recommenders enable the integration of external data sources, providing valuable insights from anticipated traffic patterns or scheduled batch jobs. Using recommenders that operate on various time scales allows for both short-term adjustments and long-term capacity planning. This multi-faceted approach can lead to more efficient resource utilization, reducing costs while maintaining the application performance. Also, custom recommenders can be designed to consider factors like node affinity, pod disruption budgets, or specific application behaviors that may not be captured by default recommenders. By using multiple recommenders, you create a more robust and adaptive system.","title":"Why Use Multiple Recommenders?"},{"location":"vpa/#implementing-multiple-recommenders","text":"This approach involves creating custom recommenders using the Kubernetes Metrics API and VPA APIs, implemented as separate microservices within your cluster. A crucial component in this setup is the Recommendation Aggregator, which combines inputs from various recommenders and makes final decisions based on predefined rules or machine learning models. To integrate this system with the VPA, you'll need to modify the VPA controller to use your custom aggregator instead of the default recommender, which requires changes to the VPA source code and rebuilding its components. Let\u2019s take an example of an e-commerce application, you might have one recommender analyzing historical sales data to predict resource needs for upcoming events, another monitoring real-time user traffic for immediate adjustments, and a third incorporating cost data to ensure recommendations stay within budget constraints. The aggregator would then synthesize these inputs to provide a balanced recommendation that optimizes both performance and cost. It's important to thoroughly test this custom setup in a non-production environment before deployment. This approach can lead to more accurate, context-aware resource optimization, making improvements in both cost efficiency and application performance. You can have a look here for more information. 6. Integrating VPA with Cluster Autoscaler or Karpenter: To achieve true cost optimization, it's crucial to integrate VPA with cluster-level autoscaling solutions like Cluster Autoscaler or Karpenter. This integration ensures that not only are your pods right-sized, but your overall cluster resources are also optimized. Why is this integration important? VPA focuses on optimizing resources at the pod level, but it doesn't directly influence the number or size of nodes in your cluster. Without proper cluster-level autoscaling, you might end up with right-sized pods but underutilized or overloaded nodes, which doesn't translate to real cost savings. Integrating with Cluster Autoscaler: Integrating VPA with Cluster Autoscaler requires an understanding of their interplay and configuration. Cluster Autoscaler responds to pod scheduling needs by adding or removing nodes, which can be indirectly triggered by VPA's resource adjustments. To optimize this integration, ensure your Cluster Autoscaler is configured with appropriate min/max node counts and scale-down delays to prevent unnecessary scaling actions that could lead to increased costs. Implement Kubernetes Pod Priority and Preemption to ensure critical pods are scheduled even during cluster scaling events. Closely monitor the interaction between VPA and Cluster Autoscaler, looking for patterns where VPA adjustments consistently trigger scaling events, and adjust configurations to minimize unnecessary infrastructure costs. This integrated approach will lead to a more cost-effective Kubernetes environment with optimized resource utilization. Integrating with Karpenter: Karpenter is a more flexible and efficient cluster autoscaler for Kubernetes. Integrating VPA with Karpenter can lead to even more optimized resource utilization and cost savings: 1. Karpenter's Fast Node Provisioning: Karpenter can quickly provision nodes that precisely match pod requirements, reducing overprovisioning and associated costs. This works well with VPA's dynamic resource adjustments to ensure optimal resource allocation. 2. Use Karpenter Provisioners: Configure Karpenter Provisioners to align with your VPA strategies and cost objectives. Create provisioners optimized for specific workload types that VPA is managing, ensuring the most cost-effective instance types are used. 3. Implement Consolidation: Take advantage of Karpenter's consolidation feature, which can move pods to optimize node utilization. This complements VPA's pod-level optimizations, further reducing infrastructure costs by eliminating underutilized nodes. 4. Custom Resource Management: Utilize Karpenter's support for custom resources in conjunction with VPA to manage specialized workloads more effectively, ensuring that expensive custom resources are allocated efficiently. To effectively integrate VPA and cluster autoscaling, adopt a gradual implementation approach, starting with separate implementations and progressively integrating them for non-critical workloads. Set conservative thresholds initially to prevent rapid fluctuations, adjusting as system stability and cost benefits are confirmed. Implement monitoring using tools like Prometheus and Grafana to track both pod-level and node-level metrics. Utilize Kubernetes simulation tools to model the impact of VPA and autoscaling decisions before production deployment. Design your system to handle potential failure scenarios, incorporating manual override procedures and fallback configurations to maintain system resilience and cost efficiency.","title":"Implementing Multiple Recommenders:"}]}